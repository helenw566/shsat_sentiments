{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c40445f",
   "metadata": {},
   "source": [
    "## 04 Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22848d22",
   "metadata": {},
   "source": [
    "This script takes in data from \\data and performs stance detection. Base code taken from [hugging face](https://huggingface.co/rwillh11/mdeberta_NLI_stance_NoContext), with modifications added to (1) average across target synonyms and (2) work with our data. Note that torch will not import with Python 3.11--switched tp Python 3.10.9 for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308c61c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aede19",
   "metadata": {},
   "source": [
    "### Off the Shelf Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16a99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv(\"../data/scraping_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41905da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "model_name = \"rwillh11/mdeberta_NLI_stance_NoContext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea1e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target and hypotheses\n",
    "stance_list = [\"positive\", \"negative\", \"neutral\"]\n",
    "target_group = [\"shsat\", \"test\", \"exam\", \"specialized high school\", \"specialized high schools\"]\n",
    "\n",
    "#dict of hypothesis for each stance and word\n",
    "hypotheses_list = dict.fromkeys(stance_list)\n",
    "\n",
    "for stance in stance_list:\n",
    "    hyp = {}\n",
    "    for target_word in target_group:\n",
    "        hyp[target_word] = f\"The text is {stance} towards {target_word}.\"\n",
    "\n",
    "    hypotheses_list[stance] = hyp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a871d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stance_detection(headline, hypotheses):\n",
    "    \"\"\"\n",
    "    takes in a headline and computes probability of each\n",
    "    hypothesis being correct. returns probabilities for \n",
    "    each stance as well as final detected stance\n",
    "    \"\"\"\n",
    "    #dictionary of results\n",
    "    results = {}\n",
    "\n",
    "    #compute prob of stance for each hypothesis\n",
    "    with torch.no_grad():\n",
    "        for stance, hyp_list in hypotheses.items():\n",
    "            temp_results = {}\n",
    "            for word, hyp  in hyp_list.items():\n",
    "                inputs = tokenizer(headline, hyp, return_tensors=\"pt\", truncation=True)\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                temp_results[word] = probs[0][0].item()  \n",
    "            #average prob across target words\n",
    "            results[stance] = max(temp_results.values()) \n",
    "\n",
    "    #select stance with highest entailment probability\n",
    "    results.update({\"max\": max(results, key = results.get)})\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de33939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict for storage\n",
    "stance_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be1eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "#get stance for each text\n",
    "for row in range(0, len(df)):\n",
    "    temp = stance_detection(df[\"full_text\"].iloc[row], hypotheses = hypotheses_list)\n",
    "    stance_dict[df[\"text_id\"].iloc[row]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44281d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform into df\n",
    "stance_df = pd.DataFrame.from_dict(stance_dict, orient=\"index\")\n",
    "stance_df.index.name = \"text_id\"\n",
    "stance_df = stance_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e778a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge it with text data\n",
    "df_stance = df.merge(stance_df, on = \"text_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95bceba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df\n",
    "df_stance.to_csv(\"../data/stance_detection_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758668a",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a4085b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df_stance = pd.read_csv(\"../data/stance_detection_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0698eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation dataset\n",
    "validation_df = df_stance[[\"text_id\", \"full_text\"]].sample(200)\n",
    "\n",
    "validation_df[\"Coder\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a33719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data\n",
    "validation_df.to_csv(\"../validation/validation_uncoded.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
