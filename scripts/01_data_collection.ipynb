{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc2871c",
   "metadata": {},
   "source": [
    "## 01 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e46c39",
   "metadata": {},
   "source": [
    "This script collects headlines and related metadata from NYC-local newsouts on articles that cover NYC's Specialized High School Exam (SHSAT). It collects data via either API calls or webscraping, wrangles data into a uniform structure, and saves the results into /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api calls\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#webscraping\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#selenium\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#data wrangling\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec396295",
   "metadata": {},
   "source": [
    "#### New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load api key\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('nyt_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec06aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url for api\n",
    "base_url =  \"https://api.nytimes.com/svc/search/v2/articlesearch.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters\n",
    "search = \"SHSAT OR Specialized High Schools Admissions Test OR Specialized High Schools Admissions Exam\"\n",
    "start = \"20120101\"\n",
    "end = \"20251118\"\n",
    "pg = 0\n",
    "counter = True\n",
    "\n",
    "#data storage\n",
    "nyt_json = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api query with pagination\n",
    "while counter == True:\n",
    "    #request\n",
    "    response = requests.get(base_url, params= {\n",
    "        \"q\": search, \n",
    "        \"api-key\": api_key,\n",
    "        \"begin_date\": start,  \n",
    "        \"end_date\": end,\n",
    "        \"page\": pg\n",
    "    })\n",
    "\n",
    "    #check status\n",
    "    if response.status_code != 200:\n",
    "        print(\"query failed\")\n",
    "        break\n",
    "    \n",
    "    #save results\n",
    "    nyt_json.append(response.json()[\"response\"][\"docs\"])\n",
    "\n",
    "    #pagination\n",
    "    if not response.json().get(\"response\", {}).get(\"docs\"):\n",
    "        counter = False\n",
    "    else:\n",
    "        pg = pg + 1\n",
    "\n",
    "    #pause to respect rate limits\n",
    "    time.sleep(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6432ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "nyt_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for page in nyt_json:\n",
    "    #skip if page is blank\n",
    "    if not page:\n",
    "            continue \n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(nyt_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for story in page:\n",
    "        pg_dict[\"link\"] = [story[\"web_url\"]]\n",
    "        pg_dict[\"headline\"] = [story[\"headline\"][\"main\"]]\n",
    "        pg_dict[\"author\"] = [story[\"byline\"][\"original\"]]\n",
    "        pg_dict[\"date_published\"] = [story[\"pub_date\"]]\n",
    "        pg_dict[\"snippet\"] = [story[\"abstract\"]]\n",
    "\n",
    "        nyt_df = pd.concat([nyt_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425613ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "nyt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "nyt_df.to_csv(\"../data/nyt_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975ecc2",
   "metadata": {},
   "source": [
    "#### Functions for Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_url(newsout: str, pg):\n",
    "    \"\"\"\n",
    "    returns specific query url for each\n",
    "    outlet and page\n",
    "    \"\"\"\n",
    "    if newsout == \"NYP\":\n",
    "    \n",
    "        base_url = \"https://nypost.com/search/SHSAT/\"\n",
    "\n",
    "        query = base_url + \"page/\" + str(pg) + \"/?orderby=relevance\"\n",
    "\n",
    "    elif newsout == \"NYDN\":\n",
    "        \n",
    "        base_url = \"https://www.nydailynews.com/page/\"\n",
    "\n",
    "        query = base_url + str(pg) + \"/?s=SHSAT&orderby=date&order=desc\"\n",
    "    \n",
    "    elif newsout == \"NY1\":\n",
    "\n",
    "        base_url = \"https://ny1.com/nyc/all-boroughs/search#SHSAT/\"\n",
    "\n",
    "        query = base_url + str(pg) + \"/publishDate%20desc\"\n",
    "\n",
    "    elif newsout == \"Brooklyn\":\n",
    "        base_url = \"https://brooklyneagle.com/page/\"\n",
    "\n",
    "        query = base_url + str(pg) + \"/?s=SHSAT\"\n",
    "    \n",
    "    elif newsout == \"City\":\n",
    "        base_url = \"https://www.city-journal.org/search?top=true&limit=12&page-number=\"\n",
    "        \n",
    "        query = base_url + str(pg) + \"&search=SHSAT&types%5B%5D=article&dates=&sort=desc\"\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_results(newsout: str, pg_soup):\n",
    "    \"\"\"\n",
    "    checks how much total headlines\n",
    "    there are for the search\n",
    "    \"\"\"\n",
    "    if newsout == \"NYP\":\n",
    "        headline_count = int(pg_soup.find(\"div\", class_ = \"search-results__header\").find(\"h2\").find(\"em\").text)\n",
    "        \n",
    "    elif newsout == \"NYDN\":\n",
    "        result_text = pg_soup.find(\"div\", class_ = \"sort-filter\").find(\"span\", class_ = \"results\").get_text()\n",
    "\n",
    "        headline_count = int(\"\".join([char for char in result_text if char.isdigit()]))\n",
    "    \n",
    "    return headline_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_results(newsout: str, pg_soup):\n",
    "    \"\"\"\n",
    "    checks how much total headlines\n",
    "    there are for the current page\n",
    "    \"\"\"\n",
    "    if newsout == \"NYP\":\n",
    "        page_results = len(pg_soup.find(\"div\", class_ = \"page__content search-results\").find(\"div\", \"search-results__stories\").find_all(\"div\", class_ = \"search-results__story\"))\n",
    "        \n",
    "    elif newsout == \"NYDN\":\n",
    "        page_results = len(pg_soup.find(\"div\", class_ = \"content-wrapper\").find(\"div\", class_ = \"search-content filter-open load-more-wrapper\").find_all(\"article\"))\n",
    "\n",
    "    return page_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c683d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping(newsout: str):\n",
    "    \"\"\"\n",
    "    scrapes newspaper outlets based on\n",
    "    newsout arg and returns list  \n",
    "    this is only for static news outlets\n",
    "    \"\"\"\n",
    "\n",
    "    #set parameters\n",
    "    pg = 1\n",
    "    stop = False\n",
    "    headline_count = None\n",
    "\n",
    "    #storage\n",
    "    pages = []\n",
    "\n",
    "    #scraping with pagination\n",
    "    while stop == False:\n",
    "        #base_url\n",
    "        query = query_url(newsout, pg)\n",
    "\n",
    "        #scrape page\n",
    "        response = requests.get(query)\n",
    "\n",
    "        #check status\n",
    "        if response.status_code != 200:\n",
    "            print(\"query failed\")\n",
    "            break\n",
    "        \n",
    "        #retrieve html\n",
    "        pg_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        #save results\n",
    "        pages.append(pg_soup)\n",
    "\n",
    "        #see how many total results there are\n",
    "        if pg == 1:\n",
    "            headline_count = total_results(newsout, pg_soup)\n",
    "\n",
    "        #see total results on current page\n",
    "        page_result = page_results(newsout, pg_soup)\n",
    "        \n",
    "        #update\n",
    "        headline_count = headline_count - page_result\n",
    "        if headline_count > 0:\n",
    "            pg = pg + 1\n",
    "        else: \n",
    "            stop = True\n",
    "\n",
    "        #pause to respect rate limits\n",
    "        time.sleep(13)\n",
    "    \n",
    "    #return scrapped results\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67a121",
   "metadata": {},
   "source": [
    "#### New York Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape data\n",
    "nyp_pages = web_scraping(\"NYP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca292ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "nyp_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1096120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for page in nyp_pages:\n",
    "    #get results for each page\n",
    "    pg_results = page.find(\"div\", class_ = \"page__content search-results\").find(\"div\", \"search-results__stories\").find_all(\"div\", class_ = \"search-results__story\")\n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(nyp_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for story in pg_results:\n",
    "        pg_dict[\"link\"] = [story.find(\"a\")[\"href\"]]\n",
    "        pg_dict[\"headline\"] = [story.find(\"h3\").get_text(strip = True)]\n",
    "        pg_dict[\"author\"] = [story.find(\"span\").get_text().split('\\xa0')[0].strip().replace(\"By\", \"\").strip()]\n",
    "        pg_dict[\"date_published\"] = [story.find(\"span\").get_text().split('\\xa0')[1].strip().replace('\\n', '').replace('\\t', '').replace(\"|\", \"\")]\n",
    "        pg_dict[\"snippet\"] = [story.find(\"p\").get_text(strip = True)]\n",
    "\n",
    "        nyp_df = pd.concat([nyp_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b34f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "nyp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "nyp_df.to_csv(\"../data/nyp_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f23934",
   "metadata": {},
   "source": [
    "#### New York Daily News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ba31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape data\n",
    "nydn_pages = web_scraping(\"NYDN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "nydn_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c727fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for page in nydn_pages:\n",
    "    #get results for each page\n",
    "    pg_results = page.find(\"div\", class_ = \"content-wrapper\").find(\"div\", class_ = \"search-content filter-open load-more-wrapper\").find_all(\"article\")\n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(nydn_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for story in pg_results:\n",
    "        pg_dict[\"link\"] = [story.find(\"a\")[\"href\"]]\n",
    "        pg_dict[\"headline\"] = [story.find(\"h2\").find(\"a\", class_ = \"article-title\").find(\"span\").get_text(strip = True)]\n",
    "        pg_dict[\"author\"] = [story.find(\"div\", class_ = \"entry-meta\").find(\"div\", class_ = \"byline\").find(\"a\").get_text()]\n",
    "        pg_dict[\"date_published\"] = [story.find(\"div\", class_ = \"entry-meta\").find(\"time\")[\"datetime\"]]\n",
    "        pg_dict[\"snippet\"] = [story.find(\"div\", class_ = \"excerpt\").get_text(strip = True)]\n",
    "\n",
    "        nydn_df = pd.concat([nydn_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "nydn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "nydn_df.to_csv(\"../data/nydn_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1dcde6",
   "metadata": {},
   "source": [
    "#### Chalkbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7420ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigate to chalkbeat search\n",
    "query = \"https://www.chalkbeat.org/search/?query=SHSAT\"\n",
    "\n",
    "driver.get(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing html per page\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ac015",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #append page html to list\n",
    "    pages.append(driver.page_source)\n",
    "\n",
    "    #find and click next button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, \"a.next_btn\")\n",
    "        if not next_button.is_displayed():\n",
    "            break\n",
    "        next_button.click()\n",
    "        time.sleep(3)\n",
    "    except NoSuchElementException:\n",
    "        print(\"finished!\")\n",
    "        break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "chalkbeat_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for pg in pages:\n",
    "    #get results for each page\n",
    "    pg_results = BeautifulSoup(pg, 'html.parser').find_all(\"div\", class_ = \"queryly_item_row\")\n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(chalkbeat_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for story in pg_results:\n",
    "        pg_dict[\"link\"] = [\"https://www.chalkbeat.org\" + story.find(\"a\")[\"href\"]]\n",
    "        pg_dict[\"headline\"] = [story.find(\"div\", class_ = \"queryly_item_title\").get_text(strip = True)]\n",
    "        pg_dict[\"date_published\"] = [story.find(\"div\", style = \"margin-top:6px;color:#555;font-size:12px;\").get_text(strip = True)]\n",
    "        pg_dict[\"snippet\"] = [story.find(\"div\", class_ = \"queryly_item_description\").get_text(strip = True)]\n",
    "\n",
    "        chalkbeat_df = pd.concat([chalkbeat_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a17879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "chalkbeat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "chalkbeat_df.to_csv(\"../data/chalkbeat_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b232b",
   "metadata": {},
   "source": [
    "#### Spectrum NY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing html per page\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pg in range(1,6):\n",
    "    #find search page\n",
    "    query = query_url(\"NY1\", pg)\n",
    "    driver.get(query)\n",
    "\n",
    "    #let page load\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        #get article html \n",
    "        hits = driver.find_elements(By.CLASS_NAME, \"hit\")\n",
    "\n",
    "        #extract hits\n",
    "        page_html = []\n",
    "        for h in hits:\n",
    "            try:\n",
    "                page_html.append(h.get_attribute(\"outerHTML\"))\n",
    "            except StaleElementReferenceException:\n",
    "                continue  \n",
    "        #store\n",
    "        if page_html:  \n",
    "            pages.append(page_html)\n",
    "\n",
    "    except (NoSuchElementException, StaleElementReferenceException):\n",
    "        continue\n",
    "    \n",
    "    #rest\n",
    "    time.sleep(3)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a48ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "spectrum_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d658c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for pg in pages:\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(spectrum_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for result in pg:\n",
    "        story = BeautifulSoup(result,'html.parser')\n",
    "        pg_dict[\"link\"] = [\"https://ny1.com\" + story.find(\"a\")[\"href\"]]\n",
    "        pg_dict[\"headline\"] = [story.find(\"div\", class_ = \"title\").get_text(strip = True)]\n",
    "        pg_dict[\"date_published\"] = [story.find(\"span\").get_text(strip = True)]\n",
    "        pg_dict[\"snippet\"] = [story.find(\"div\", class_ = \"description\").get_text(strip = True)]\n",
    "\n",
    "        spectrum_df = pd.concat([spectrum_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "spectrum_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "spectrum_df.to_csv(\"../data/spectrum_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d36843",
   "metadata": {},
   "source": [
    "#### amNY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00feb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run query\n",
    "response = requests.get(\n",
    "    \"https://www.amny.com/?s=SHSAT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe61492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html\n",
    "pg_soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all articles\n",
    "stories = pg_soup.find(\"main\").find_all(\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "amny_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for story in stories:\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(amny_df.columns)\n",
    "    \n",
    "    #extract data\n",
    "    pg_dict[\"link\"] = [story.find(\"a\")[\"href\"]]\n",
    "    pg_dict[\"headline\"] = [story.find(\"a\")[\"title\"]]\n",
    "    if story.find(\"span\", class_ = \"posted-on\"):\n",
    "        pg_dict[\"date_published\"] = [story.find(\"span\", class_ = \"posted-on\").find(\"time\").get_text()]\n",
    "\n",
    "    amny_df = pd.concat([amny_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "amny_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "amny_df.to_csv(\"../data/amny_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7e1ff",
   "metadata": {},
   "source": [
    "#### Brooklyn Daily Eagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium/driver, using uc to avoid cloudflare issues\n",
    "driver = uc.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#html storage\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping pages\n",
    "for num in range(1, 11):\n",
    "    #go to page\n",
    "    driver.get(query_url(\"Brooklyn\", num))\n",
    "    #grab html\n",
    "    pages.append(driver.page_source)\n",
    "    #rest\n",
    "    time.sleep(3)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd42525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "brooklyn_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for pg in pages:\n",
    "    #get bs4 obj\n",
    "    pg_soup = BeautifulSoup(pg, \"html.parser\")\n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(brooklyn_df.columns)\n",
    "\n",
    "    #identify article list\n",
    "    if pg_soup.find(\"div\", class_ = \"main-container\"):\n",
    "        article_list = pg_soup.find(\"div\", class_ = \"main-container\").find_all(\"article\")\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    #loop through each story\n",
    "    for result in article_list:\n",
    "        pg_dict[\"link\"] = [result.find(\"a\")[\"href\"]] if result.find(\"a\") else [None]\n",
    "        pg_dict[\"headline\"] = [result.find(\"a\").get(\"title\")] if result.find(\"a\") else [None]\n",
    "        pg_dict[\"date_published\"] = [result.find(class_ = \"meta\").get_text(strip = True)] if result.find(class_ = \"meta\") else [None]\n",
    "        pg_dict[\"snippet\"] = [result.find(\"div\", class_=\"meta\").find_next_sibling(text = True).strip()] if result.find(class_ = \"meta\") else [None]\n",
    "\n",
    "        brooklyn_df = pd.concat([brooklyn_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "brooklyn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "brooklyn_df.to_csv(\"../data/brooklyn_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980f106",
   "metadata": {},
   "source": [
    "### Queens Chronicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base url\n",
    "base = \"https://www.qchron.com/search/?tncms_csrf_token=c4d400dd4c6760f075f26f98b75ee774e2ece127677b3054cf9964704481bab4.0e7fd1a70149b3d138a0&l=25&sort=relevance&f=html&t=article%2Cvideo%2Cyoutube%2Ccollection&app=editorial&nsa=eedition&q=SHSAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b517a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get url for each page using offset\n",
    "query_urls = [base + \"&o=\" + str(num) for num in range(0, 125, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storage\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96769592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#webscraping\n",
    "for pg_url in query_urls:\n",
    "    #query page\n",
    "    driver.get(pg_url)\n",
    "    #get page source\n",
    "    pg = driver.page_source\n",
    "    #extract html\n",
    "    response = BeautifulSoup(pg, 'html.parser').find_all(\"div\", class_ = \"card-container\")\n",
    "    #append\n",
    "    pages.append(response)\n",
    "    #rest\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc344fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close selenium\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "queens_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for pg in pages:\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(queens_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for result in pg:\n",
    "        pg_dict[\"link\"] = [\"https://www.qchron.com/\" + result.find(\"a\")[\"href\"]] if result.find(\"a\") else [None]\n",
    "        pg_dict[\"headline\"] = [result.find(\"div\", class_ = \"card-headline\").get_text(strip = True)] if result.find(\"div\", class_ = \"card-headline\") else [None]\n",
    "        pg_dict[\"date_published\"] = [result.find(\"li\", class_ = \"card-date\").find(\"time\")[\"datetime\"]] if result.find(\"li\", class_ = \"card-date\").find(\"time\") else [None]\n",
    "        pg_dict[\"snippet\"] = [result.find(\"div\", class_ = \"card-lead\").get_text(strip = True)]\n",
    "\n",
    "        queens_df = pd.concat([queens_df, pd.DataFrame(pg_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "queens_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "queens_df.to_csv(\"../data/queens_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56030ee3",
   "metadata": {},
   "source": [
    "### NY Amsterdam News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a60d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get website\n",
    "query = \"https://amsterdamnews.com/\"\n",
    "driver.get(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find search button\n",
    "search_button = driver.find_element(By.ID, \"search-toggle\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74234ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for SHSAT : THIS ONLY WORKS IF BROWSER IS EXPANDED FOR SOME REASON\n",
    "search_box = driver.find_element(By.ID, \"search-form-2\")\n",
    "search_box.send_keys(\"SHSAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter results\n",
    "search_box.send_keys(Keys.ENTER)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ce5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find popout where the search results are\n",
    "results = driver.find_element(By.CLASS_NAME, \"jetpack-instant-search__search-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll until we run out of results\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"arguments[0].scrollBy(0, 1000);\", results)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs page results\n",
    "news_page = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e48f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quits selenium\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls out list of articles\n",
    "article_list = BeautifulSoup(news_page, 'html.parser').find(\"div\", class_ = \"jetpack-instant-search__search-results\").find(\"ol\", class_ = \"jetpack-instant-search__search-results-list is-format-expanded\").find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "nyamsterdam_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88896905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for art in article_list:\n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(nyamsterdam_df.columns)\n",
    "    \n",
    "    #extract data\n",
    "    pg_dict[\"link\"] = [\"https:\" + art.find(\"div\").find(\"a\")[\"href\"]] if art.find(\"div\") else [None]\n",
    "    pg_dict[\"headline\"] = [art.find(\"div\").find(\"a\").get_text()] if art.find(\"div\") else [None]\n",
    "    pg_dict[\"date_published\"] = [art.find_all(class_ = \"jetpack-instant-search__path-breadcrumb-piece\")[1].get_text(strip = True)] if len(art.find_all(class_ = \"jetpack-instant-search__path-breadcrumb-piece\")) > 1 else [None]\n",
    "    pg_dict[\"snippet\"] = [art.find(\"div\", class_ = \"jetpack-instant-search__search-result-expanded__content\").get_text(strip = True)] if art.find(\"div\", class_ = \"jetpack-instant-search__search-result-expanded__content\") else [None]\n",
    "\n",
    "    nyamsterdam_df = pd.concat([nyamsterdam_df, pd.DataFrame(pg_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e80920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empties\n",
    "nyamsterdam_df = nyamsterdam_df.dropna(axis = 0, how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec58b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "nyamsterdam_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a36b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "nyamsterdam_df.to_csv(\"../data/nyamsterdam_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f62f09",
   "metadata": {},
   "source": [
    "### City Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b618b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty storage\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraping\n",
    "for pg in range(1,3):\n",
    "    query = query_url(\"City\", pg)\n",
    "    driver.get(query)\n",
    "    pages.append(driver.page_source)\n",
    "    time.sleep(3)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ec223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "city_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e080598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for news_page in pages:\n",
    "    article_list = BeautifulSoup(news_page, 'html.parser').find_all(\"div\", class_=\"m_card horizontal with-thumbnail vertical\")\n",
    "    for art in article_list:\n",
    "\n",
    "        #create temp dictionary\n",
    "        pg_dict = dict.fromkeys(city_df.columns)\n",
    "        \n",
    "        #extract data\n",
    "        pg_dict[\"link\"] = [art.find(\"a\", class_ = \"title\").get(\"href\")]\n",
    "        pg_dict[\"headline\"] = [art.find(\"a\", class_ = \"title\").get_text()]\n",
    "        pg_dict[\"date_published\"] = [art.find(\"div\", class_ = \"date\").get_text()]\n",
    "        pg_dict[\"author\"] = [art.find(\"span\", class_ = \"authors\").get_text()]\n",
    "        pg_dict[\"snippet\"] = [art.find(\"p\", class_ = \"subtitle\").get_text()]\n",
    "\n",
    "        city_df = pd.concat([city_df, pd.DataFrame(pg_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results\n",
    "city_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0558c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "city_df.to_csv(\"../data/cityjournal_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749ec81",
   "metadata": {},
   "source": [
    "### Gotham Gazette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get api key for google custom search\n",
    "api_key = os.environ.get('cse_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716de210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url for api\n",
    "base_url =  \"https://www.googleapis.com/customsearch/v1?\"\n",
    "pages = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify starting point\n",
    "start_page = 1\n",
    "counter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "while counter:\n",
    "    response = requests.get(base_url, params= {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": \"016666263354593363178:zqqt8gavlkw\",\n",
    "        \"q\": \"SHSAT\",\n",
    "        \"start\": start_page\n",
    "    })\n",
    "    #check status\n",
    "    if response.status_code != 200:\n",
    "        print(\"query failed\")\n",
    "        break\n",
    "    \n",
    "    #append results\n",
    "    if response.json().get(\"items\") and response.json()[\"queries\"].get(\"nextPage\"):\n",
    "        pages.append(response.json()[\"items\"])\n",
    "        #update\n",
    "        start_page = response.json()[\"queries\"][\"nextPage\"][0][\"startIndex\"]\n",
    "    \n",
    "    #continue loop?\n",
    "    if response.json()[\"queries\"].get(\"nextPage\"):\n",
    "        counter = True\n",
    "    else:\n",
    "        counter = False\n",
    "    #rest \n",
    "    time.sleep(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty df\n",
    "gotham_df = pd.DataFrame(\n",
    "    columns = [\"link\", \"headline\" , \"author\", \"date_published\", \"snippet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "for pg in pages:\n",
    "    #skip if page is blank\n",
    "    if not pg:\n",
    "            continue \n",
    "\n",
    "    #create temp dictionary\n",
    "    pg_dict = dict.fromkeys(gotham_df.columns)\n",
    "    \n",
    "    #loop through each story\n",
    "    for story in pg:\n",
    "        pg_dict[\"link\"] = [story[\"link\"]]\n",
    "        pg_dict[\"headline\"] = [story[\"pagemap\"][\"metatags\"][0].get(\"og:title\")]\n",
    "        pg_dict[\"author\"] = [story[\"pagemap\"][\"metatags\"][0].get(\"author\")]\n",
    "        pg_dict[\"date_published\"] = [story[\"snippet\"].split(\" ...\")[0]]\n",
    "        pg_dict[\"snippet\"] = [story[\"snippet\"].split(\" ...\")[1]] if len(story[\"snippet\"].split(\" ...\")) > 1 else [story[\"snippet\"].split(\" ...\")[0]]\n",
    "\n",
    "        gotham_df = pd.concat([gotham_df, pd.DataFrame(pg_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "gotham_df.to_csv(\"../data/gotham_results.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
